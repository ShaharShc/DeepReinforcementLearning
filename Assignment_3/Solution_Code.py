# -*- coding: utf-8 -*-
"""Assignment 3 - Meta and Transfer Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UneG_xNmDY2p9Q70WJUyk4qm4TYNPqkz

### Shahar Shcheranski 313326985

### Sarit Hollander 206172686

# **Assignment 3  - Meta and Transfer Learning**

## Imports
"""

from typing import Dict, Tuple, List
from datetime import datetime
import gym
import numpy as np
import tensorflow.compat.v1 as tf
import time
from statistics import mean
from collections import deque, namedtuple
from sklearn.preprocessing import StandardScaler

"""## **Section 1 – Training individual networks (25%)**

### **CartPole**
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('CartPole-v1')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6
action_mapping = {0: 0, 2: 1}

class ActorNetwork():
    def __init__(self, learning_rate, name='actor_network'):
        self._learning_rate = learning_rate

        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, self.state_size], name="state")
            self.action = tf.placeholder(tf.int32, [self.action_size], name="action")
            self.adv = tf.placeholder(tf.float32, name="advantage")
            self.learning_rate = tf.placeholder(tf.float32, name="lr")

            self.W1 = tf.get_variable("W1", [self.state_size, 12],
                                      initializer=tf.keras.initializers.glorot_normal(seed=0))
            self.b1 = tf.get_variable("b1", [12], initializer=tf.zeros_initializer())
            self.W2 = tf.get_variable("W2", [12, self.action_size],
                                      initializer=tf.keras.initializers.glorot_normal(seed=0))
            self.b2 = tf.get_variable("b2", [self.action_size], initializer=tf.zeros_initializer())

            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)
            self.A1 = tf.nn.relu(self.Z1)
            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)

            # Softmax probability distribution over actions
            self.actions_distribution = tf.squeeze(tf.nn.softmax(self.output))
            # Loss with negative log probability
            self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.action)
            self.loss = tf.reduce_mean(self.neg_log_prob * self.adv)
            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.actions_distribution, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess)

    def fit(self, state, action_one_hot, advantage, sess):
        _, loss = sess.run([self.optimizer, self.loss],
                           {self.state: state,
                            self.action: action_one_hot,
                            self.advantage: advantage,
                            self.learning_rate: self._learning_rate})

        return loss

    def reduce_lr(self):
        self._learning_rate *= 0.1


class CriticNetwork():
    def __init__(self, learning_rate, name='critic_network'):
        self._learning_rate = learning_rate

        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.target = tf.placeholder(tf.float32, [1], name='target')
            self.learning_rate = tf.placeholder(tf.float32, name='lr')

            x = tf.layers.dense(inputs=self.state, units=32, activation=tf.nn.relu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')
            self.output = tf.layers.dense(inputs=x, units=1, activation=None, name='output')

            self.loss = mse(y_pred=self.output, y_target=self.target)

            self.training_op = tf.train.AdamOptimizer(self.learning_rate,
                                                      name='critic_optimizer').minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target,
                            self.learning_rate: self._learning_rate})

        return loss

    def reduce_lr(self):
        self._learning_rate *= 0.1


def mse(y_pred, y_target):
    return tf.reduce_mean(tf.pow(y_pred - y_target, 2))


def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action):
    action = action_mapping[action] if action in action_mapping else env.action_space.sample()
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state):
    return np.pad(state, (0, state_size - state.shape[0]))


def sample_space(env):
    return adapt_state(env.observation_space.sample())


def run_evaluate(env, actor, episodes, sess=None):
    sess = sess or tf.get_default_session()

    episode_rewards = []
    average_rewards = float('-inf')

    for i in range(episodes):

        state = reset_env(env)
        state = state.reshape([1, state_size])
        episode_transitions = []

        for _ in range(env.spec.max_episode_steps):
            actions_distribution = actor.estimate(state, sess=sess)
            action: int = np.argmax(actions_distribution)

            next_state, reward, done, _ = step_env(env, action)
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward,
                                    next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}"
          .format(episodes, average_rewards))

    return average_rewards


def run(env, actor, critic, save_path=None):
    max_episodes = 50000
    min_episodes_queue = 100

    discount_factor = 0.99

    summary_writer = tf.summary.FileWriter('./logs/CartPole')
    step_counter = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        total_reward = 0
        for episode in range(max_episodes):
            state = reset_env(env)
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                actions_distribution = actor.estimate(state, sess=sess)
                action = np.random.choice(np.arange(len(actions_distribution)), p=actions_distribution)
                next_state, reward, done, _ = step_env(env, action)
                next_state = next_state.reshape([1, state_size])

                total_reward += reward

                step_summary.value.add(tag='step_reward', simple_value=reward)

                action_one_hot = np.zeros(action_size)
                action_one_hot[action] = 1
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action_one_hot, reward=reward,
                                        next_state=next_state, done=done)
                episode_transitions.append(transition)

                state_val = critic.estimate(state, sess=sess)

                next_state_val = 0 if done else critic.estimate(next_state, sess=sess)

                target = reward + discount_factor * next_state_val
                advantage = target - state_val

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess=sess)
                actor_loss = actor.fit(state, action_one_hot, advantage * I, sess=sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)
                summary_writer.add_summary(step_summary, step_counter)

                step_counter += 1

                if done:
                    if total_reward < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, fail"
                              .format(episode, total_reward, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, actor, episodes=15, sess=sess)
                        episode_summary.value.add(tag='evaluation_average_rewards', simple_value=evaluate)
                        solved = episode_reward > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state

            if total_reward < 0:
                print("Episode {}, Reinitialize network weight".format(episode))
                sess.run(tf.global_variables_initializer())
                total_reward = 0

            if solved:
                if save_path is not None:
                    saver = tf.train.Saver()
                    saver.save(sess, save_path, global_step=step_counter)
                break

    summary_writer.flush()


if __name__ == '__main__':
    tf.reset_default_graph()

    actor = ActorNetwork(learning_rate=0.0004)
    critic = CriticNetwork(learning_rate=0.005)

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, actor, critic, save_path='cartpole/cartpole')
    print("--- %s seconds ---" % (time.time() - start_time))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/CartPole'

"""### **Acrobot**

"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('Acrobot-v1')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6
action_mapping={0: 0, 1: 1, 2: 2}


class ActorNetwork:
    def __init__(self, state_size, action_size, learning_rate, name='acrobot_actor_network'):
        self.state_size = state_size
        self.action_size = action_size

        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, self.state_size], name="state")
            self.advantage = tf.placeholder(tf.float32, [1], name='advantage')
            self.action = tf.placeholder(tf.float32, name="action")

            self.base = tf.layers.dense(inputs=self.state, units=40, activation=tf.nn.relu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')

            self.output = tf.layers.dense(inputs=self.base, units=self.action_size, activation=tf.nn.softmax,
                                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                          name='output')
            
            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.action)
            self.loss = tf.reduce_mean(neg_log_prob * self.advantage)

            self.training_op = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                                      name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess: tf.Session):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess: tf.Session):
        return self.predict(state, sess)[0]

    def fit(self, state, action, advantage, sess: tf.Session):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.advantage: advantage,
                            self.action: action})

        return loss


class CriticNetwork:
    def __init__(self, state_size, learning_rate, name='acrobot_critic_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.target = tf.placeholder(tf.float32, [1], name='target')

            self.base = tf.layers.dense(inputs=self.state, units=32, activation=tf.nn.relu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')
            self.output = tf.layers.dense(inputs=self.base, units=1, activation=None, name='output')

            self.loss = mse(y_pred=self.output, y_target=tf.stop_gradient(self.target))

            self.training_op = tf.train.AdamOptimizer(learning_rate, name='critic_optimizer').minimize(self.loss)

    def predict(self, state, sess: tf.Session):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess: tf.Session):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess: tf.Session):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target})
        return loss

def mse(y_pred, y_target):
    return tf.reduce_mean(tf.pow(y_pred - y_target, 2))

def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action, use_mapping):
    if use_mapping:
      if action in action_mapping:
        action = action_mapping[action]
      else: 
        action = env.action_space.sample()
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state: np.ndarray):
    return np.pad(state, (0, state_size - state.shape[0]))

def run_evaluate(env, actor, episodes=100, sess=None):
    sess = sess or tf.get_default_session()

    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = state.reshape([1, state_size])
        episode_transitions = []

        for j in range(env.spec.max_episode_steps):
            action_dist = actor.estimate(state, sess)
            action: int = np.argmax(action_dist)

            next_state, reward, done, _ = step_env(env, action, False)
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward, next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))
    
    return average_rewards


def run(env, actor, critic):

    max_episodes = 5000
    min_episodes_queue = 100
    discount_factor = 0.99

    summary_writer = tf.summary.FileWriter('./logs/Acrobot')

    steps_count = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        total_rewards = 0

        for episode in range(max_episodes):
            state = reset_env(env)
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                action_dist = actor.estimate(state, sess)
                action = np.random.choice(np.arange(len(action_dist)), p=action_dist)

                next_state, reward, done, _ = step_env(env, action, True)
                next_state = next_state.reshape([1, state_size])

                step_summary.value.add(tag='step_reward', simple_value=reward)

                action_one_hot = np.zeros(action_size)
                action_one_hot[action] = 1
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done)
                episode_transitions.append(transition)

                value_estimate = critic.estimate(state, sess)

                if done:
                  if step < 500:
                    reward = 100
                  else:
                    value_estimate_next = 0
                else:
                    value_estimate_next = critic.estimate(next_state, sess)

                target = reward + discount_factor * value_estimate_next
                advantage = target - value_estimate

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess)
                actor_loss = actor.fit(state, action_one_hot, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                steps_count += 1
                I = discount_factor ** I

                summary_writer.add_summary(step_summary, steps_count)

                if done:
                    if total_rewards < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, Negative".format(episode, total_rewards, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = np.mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, actor, episodes=15, sess=sess)
                        episode_summary.value.add(tag='average_rewards_evaluation', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state

            if total_rewards < 0:
                print("Episode {}, Initializing network weights".format(episode))
                sess.run(tf.global_variables_initializer())
                total_rewards = 0

            if solved:
              print(' Solved at episode: ' + str(episode))
              my_saver = tf.train.Saver()
              my_saver.save(sess, 'acrobot/acrobot', global_step=steps_count)
              break

    summary_writer.flush()


if __name__ == '__main__':
    tf.reset_default_graph()
    actor_lr = 0.0004
    critic_lr = 0.005

    actor = ActorNetwork(state_size, action_size, learning_rate=actor_lr)
    critic = CriticNetwork(state_size, learning_rate=critic_lr)

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, actor, critic)
    print("--- %s seconds ---" % (time.time() - start_time))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/Acrobot'

"""### Mountain Car Continuous"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('MountainCarContinuous-v0')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6


class ActorNetwork:
    def __init__(self, learning_rate, name='actor_mc_network'):

        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.advantage = tf.placeholder(tf.float32, [1], name='advantage')
            self.action = tf.placeholder(tf.float32, name="action")

            x = tf.layers.dense(inputs=self.state, units=40, activation=tf.nn.elu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')
            self.base = tf.layers.dense(inputs=x, units=40, activation=tf.nn.elu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='dense2')

            sigma = tf.layers.dense(inputs=self.base, units=1, activation=None, name='sigma')
            sigma = tf.nn.softplus(sigma) + Epsilon

            mu = tf.layers.dense(inputs=self.base, units=1, activation=None, name='mu')

            actions_distribution = tf.distributions.Normal(mu, sigma)
            _action = actions_distribution.sample()
            self.output = tf.clip_by_value(_action, -1, 1)
            self.loss = -tf.log(actions_distribution.prob(self.action) + Epsilon) * self.advantage

            self.training_op = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                                      name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess: tf.Session):
        return sess.run(self.output, {self.state: state})

    def update(self, state, action, advantage, sess: tf.Session):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.advantage: advantage,
                            self.action: action})

        return loss


class CriticNetwork:
    def __init__(self, learning_rate, name='critic_mc_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.target = tf.placeholder(tf.float32, [1], name='target')
            self.learning_rate = tf.placeholder(tf.float32, name='lr')

            x = tf.layers.dense(inputs=self.state, units=400, activation=tf.nn.elu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')
            self.base = tf.layers.dense(inputs=x, units=400, activation=tf.nn.elu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='dense2')
            self.output = tf.layers.dense(inputs=self.base, units=1, activation=None, name='output')

            self.loss = tf.math.squared_difference(self.output, self.target)

            self.training_op = tf.train.AdamOptimizer(learning_rate,
                                                      name='critic_optimizer').minimize(self.loss)

    def estimate(self, state, sess: tf.Session):
        return sess.run(self.output, {self.state: state})[0, 0]

    def update(self, state, target, sess: tf.Session):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target})

        return loss


def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action):
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state):
    return np.pad(state, (0, state_size - state.shape[0]))


def sample_space(env):
    return adapt_state(env.observation_space.sample())


def run_evaluate(env, scaler, actor, episodes, sess=None):
    sess = sess or tf.get_default_session()

    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = scaler.transform([state]).squeeze()
        state = state.reshape([1, state_size])
        episode_transitions = []

        for j in range(env.spec.max_episode_steps):
            action_dist = actor.predict(state, sess)
            action = float(action_dist)

            next_state, reward, done, _ = step_env(env, [action])
            next_state = scaler.transform([next_state]).squeeze()
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward,
                                    next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))
    
    return average_rewards


def run(env, actor, critic, scaler):
    observation_examples = np.array([sample_space(env) for _ in range(10000)])
    scaler.fit(observation_examples)

    max_episodes = 500
    min_episodes_queue = 100
    discount_factor = 0.99

    summary_writer = tf.summary.FileWriter('./logs/MountainCar')

    steps_count = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with tf.Session(config=tf.ConfigProto()) as sess:
        sess.run(tf.global_variables_initializer())
        total_rewards = 0
        min_episodes = 0

        for episode in range(max_episodes):

            state = reset_env(env)
            state = scaler.transform([state]).squeeze()
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                action_dist = actor.predict(state, sess)
                action = float(action_dist)

                next_state, reward, done, _ = step_env(env, [action])
                next_state = scaler.transform([next_state]).squeeze()
                next_state = next_state.reshape([1, state_size])

                total_rewards += reward

                step_summary.value.add(tag='step_reward', simple_value=reward)
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action, reward=reward,
                                        next_state=next_state, done=done)
                episode_transitions.append(transition)

                value_estimate = critic.estimate(state, sess)

                if done:
                    value_estimate_next = 0
                else:
                    value_estimate_next = critic.estimate(next_state, sess)

                target = reward + discount_factor * value_estimate_next
                advantage = target - value_estimate

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.update(state, target * I, sess)
                actor_loss = actor.update(state, action, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                steps_count += 1

                summary_writer.add_summary(step_summary, steps_count)

                if done:
                    if total_rewards < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, fail"
                              .format(episode, total_rewards, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = np.mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, scaler, actor, episodes=15, sess=sess)
                        episode_summary.value.add(tag='average_rewards_evaluation', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state
                min_episodes += 1

            if total_rewards < 0 and min_episodes > 5:
                print("Episode {}, Initializing network weights".format(episode))
                sess.run(tf.global_variables_initializer())
                total_rewards = 0
                min_episodes = 0

            if solved:
                my_saver = tf.train.Saver()
                my_saver.save(sess, 'mountainCar/mountainCar', global_step=steps_count)
                break

    summary_writer.flush()


if __name__ == '__main__':
    tf.reset_default_graph()
    actor_lr = 0.00002
    critic_lr = 0.001

    actor = ActorNetwork(learning_rate=actor_lr)
    critic = CriticNetwork(learning_rate=critic_lr)
    scaler = StandardScaler()

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, actor, critic, scaler)
    print("--- %s seconds ---" % (time.time() - start_time))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/MountainCar'

"""## **Section 2 – Fine-tune an existing model (25%)**

### acrobot -> cartpole
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('CartPole-v1')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6
action_mapping = {0:0, 2:1}


def build_base(inputs, layer_name, graph, activation=None, name=None, kernel='kernel', bias='bias'):
    d1_w = graph.get_tensor_by_name(layer_name + '/' + kernel + ':0')
    d1_b = graph.get_tensor_by_name(layer_name + '/' + bias + ':0')
    output = tf.add(tf.matmul(inputs, d1_w), d1_b, name=name)

    if activation is None:
      return output
    else:
      return activation(output)


class ActorNetwork:
    def __init__(self, state_size, action_size, learning_rate, graph, name='cart_pole_actor_network'):
        self.state_size = state_size
        self.action_size = action_size
        self.graph = graph
        self.state = self.graph.get_tensor_by_name("acrobot_actor_network/state:0")
        
        with tf.variable_scope(name):
            self.advantage = tf.placeholder(tf.float32, name='advantage')
            self.action = tf.placeholder(tf.float32, name="action")

            x = build_base(inputs=self.state, layer_name='acrobot_actor_network/base_net', graph=graph, activation=tf.nn.elu, name='dense1')
            x = tf.layers.Dropout()(x)
            self.base = tf.layers.dense(inputs=x, units=32, activation=tf.nn.elu, kernel_initializer=tf.keras.initializers.glorot_normal(seed=0), name='base_net')

            self.output = tf.layers.dense(inputs=self.base, units=self.action_size, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.glorot_normal(seed=0), name='output')
            
            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.action)
            self.loss = tf.reduce_mean(neg_log_prob * self.advantage)

            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess):
      return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
      return self.predict(state, sess)[0]

    def fit(self, state, action, advantage, sess):
      _, loss = sess.run([self.optimizer, self.loss],
                         {self.state: state,
                          self.advantage: advantage,
                          self.action: action})
      return loss

class CriticNetwork:
    def __init__(self, state_size, learning_rate, graph, name='cart_pole_critic_network'):
      self.state_size = state_size
      self.state = graph.get_tensor_by_name("acrobot_critic_network/state:0")
      self.target = graph.get_tensor_by_name('acrobot_critic_network/target:0')
      self.graph = graph

      with tf.variable_scope(name):
          self.base = build_base(inputs=self.state, layer_name='acrobot_critic_network/base_net', graph=graph, activation=tf.nn.relu, name='base_net')
          
          self.output = tf.layers.dense(inputs=self.base, units=1, activation=None, kernel_initializer=tf.keras.initializers.glorot_normal(seed=0), name='output')

          self.loss = mse(y_pred=self.output, y_target=self.target)
          self.optimizer = tf.train.AdamOptimizer(learning_rate, name='critic_optimizer').minimize(self.loss)

    def predict(self, state, sess):
      return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess):
      _, loss = sess.run([self.optimizer, self.loss],
                         {self.state: state,
                          self.target: target})
      return loss


def mse(y_pred, y_target):
    return tf.reduce_mean(tf.pow(y_pred - y_target, 2))

def reset_env(env):
    return adapt_state(env.reset())

def step_env(env, action):
    if action in action_mapping:
      action = action_mapping[action]
    else: 
      action = env.action_space.sample()
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info

def adapt_state(state):
    return np.pad(state, (0, state_size - state.shape[0]))

def sample_space(env):
    return adapt_state(env.observation_space.sample())

def run_evaluate(env, scaler, actor, episodes, sess=None):
    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = state.reshape([1, state_size])
        episode_transitions = []

        for j in range(env.spec.max_episode_steps):
            action_dist = actor.estimate(state, sess)
            action: int = np.argmax(action_dist)

            next_state, reward, done, _ = step_env(env, action)
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward, next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))
    
    return average_rewards


def run(env, actor, critic, scaler, sess):
    observation_examples = np.array([sample_space(env) for _ in range(10000)])
    scaler.fit(observation_examples)

    max_episodes = 2000
    min_episodes_queue = 100
    discount_factor = 0.99

    summary_writer = tf.summary.FileWriter('./logs/AcrobotToCartpole')

    steps_count = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with sess.as_default():
        sess.run(tf.global_variables_initializer())
        total_rewards = 0

        for episode in range(max_episodes):
            state = reset_env(env)
            state = scaler.transform([state]).squeeze()
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                action_dist = actor.estimate(state, sess)
                action = np.random.choice(np.arange(len(action_dist)), p=action_dist)

                next_state, reward, done, _ = step_env(env, action)
                next_state = scaler.transform([next_state]).squeeze()
                next_state = next_state.reshape([1, state_size])

                step_summary.value.add(tag='step_reward', simple_value=reward)

                action_one_hot = np.zeros(action_size)
                action_one_hot[action] = 1
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done)
                episode_transitions.append(transition)

                value_estimate = critic.estimate(state, sess)

                if done:
                    value_estimate_next = 0
                else:
                    value_estimate_next = critic.estimate(next_state, sess)

                target = reward + discount_factor * value_estimate_next
                advantage = target - value_estimate

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess)
                actor_loss = actor.fit(state, action_one_hot, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                steps_count += 1
                I = discount_factor ** I

                summary_writer.add_summary(step_summary, steps_count)

                if done:
                    if total_rewards < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, fail".format(episode, total_rewards, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = np.mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, scaler, actor, episodes=100, sess=sess)
                        episode_summary.value.add(tag='average_rewards_evaluation', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state

            if total_rewards < 0:
                print("Episode {}, Initializing network weights".format(episode))
                sess.run(tf.global_variables_initializer())
                total_rewards = 0

            if solved:
              print(' Solved at episode: ' + str(episode))
              break

    summary_writer.flush()


if __name__ == '__main__':
    tf.reset_default_graph()
    sess = tf.Session()
    my_saver = tf.train.import_meta_graph("acrobot/acrobot-2327.meta")
    my_saver.restore(sess, 'acrobot/acrobot-2327')
    graph = tf.get_default_graph()

    actor_lr = 0.00004
    critic_lr = 0.002

    actor = ActorNetwork(state_size, action_size, learning_rate=actor_lr, graph=graph)
    critic = CriticNetwork(state_size, learning_rate=critic_lr, graph=graph)
    scaler = StandardScaler()

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, actor, critic, scaler, sess)
    print("--- %s seconds ---" % (time.time() - start_time))

    sess.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/AcrobotToCartpole'

"""### cartploe -> mountainCar"""

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('MountainCarContinuous-v0')
np.random.seed(1)
Epsilon = 1e-5
action_size = 3
state_size = 6


def build_base(inputs, layer_name, graph, activation=None, name=None,
               kernel='kernel', bias='bias'):
    d1_w = graph.get_tensor_by_name(layer_name + '/' + kernel + ':0')
    d1_b = graph.get_tensor_by_name(layer_name + '/' + bias + ':0')
    output = tf.add(tf.matmul(inputs, d1_w), d1_b, name=name)

    if activation is None:
      return output
    else:
      return activation(output)


class ActorNetwork():
    def __init__(self, learning_rate, graph: tf.Graph, name='actor_cp_mc_network'):
        self.graph = graph

        self.state = self.graph.get_tensor_by_name("actor_network/state:0")

        with tf.variable_scope(name):
            self.action = tf.placeholder(tf.float32, name="action")
            self.advantage = tf.placeholder(tf.float32, name="advantage")

            self.base = build_base(inputs=self.state, layer_name='actor_network', graph=graph,
                                   activation=tf.nn.elu, name='base_net', kernel='W1', bias='b1')

            sigma = tf.layers.dense(inputs=self.base, units=1, activation=None, name='sigma')
            sigma = tf.nn.softplus(sigma) + Epsilon

            mu = tf.layers.dense(inputs=self.base, units=1, activation=None, name='mu')

            actions_distribution = tf.distributions.Normal(mu, sigma)
            _action = actions_distribution.sample()
            self.output = tf.clip_by_value(_action, -1, 1)
            self.loss = -tf.log(actions_distribution.prob(self.action) + Epsilon) * self.advantage

            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                                    name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess=sess)[0, 0]

    def fit(self, state, action, advantage, sess):
        _, loss = sess.run([self.optimizer, self.loss],
                           {self.state: state,
                            self.action: action,
                            self.advantage: advantage})

        return loss


class CriticNetwork():
    def __init__(self, learning_rate, graph, name='critic_cp_mc_network'):
        self.graph = graph

        self.state = graph.get_tensor_by_name("critic_network/state:0")
        self.target = graph.get_tensor_by_name('critic_network/target:0')

        with tf.variable_scope(name):
            base = build_base(inputs=self.state, layer_name='critic_network/dense1', graph=graph,
                              activation=tf.nn.relu, name='base_net')

            self.output = tf.layers.dense(inputs=base, units=1, activation=None, name='output')

            self.loss = self.mse(y_pred=self.output, y_target=self.target)
            self.training_op = tf.train.AdamOptimizer(learning_rate, name='critic_optimizer').minimize(self.loss)

    def mse(self, y_pred, y_target):
        return tf.reduce_mean(tf.pow(y_pred - y_target, 2))

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target})

        return loss


def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action):
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state: np.ndarray):
    return np.pad(state, (0, state_size - state.shape[0]))


def sample_space(env):
    return adapt_state(env.observation_space.sample())



def run_evaluate(env, actor, sess, scaler, episodes):
    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = scaler.transform([state]).squeeze()
        state = state.reshape([1, state_size])
        episode_transitions = []

        for _ in range(env.spec.max_episode_steps):
            action_dist = actor.predict(state, sess)
            action = float(action_dist)

            next_state, reward, done, _ = step_env(env, [action])
            next_state = scaler.transform([next_state]).squeeze()
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward,
                                    next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))

    return average_rewards


def run(env, sess, actor, critic, scaler):
    observation_examples = np.array([sample_space(env) for _ in range(10000)])
    scaler.fit(observation_examples)

    max_episodes = 500
    min_episodes_queue = 100
    discount_factor = 0.99
    evaluation_episodes = 15

    summary_writer = tf.summary.FileWriter('./logs/CartpoleToMountainCar')

    step_counter = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with sess.as_default():
        sess.run(tf.global_variables_initializer())

        total_reward = 0
        minimum_episodes = 0
        for episode in range(max_episodes):

            state = reset_env(env)
            state = scaler.transform([state]).squeeze()
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                action_dist = actor.predict(state, sess)
                action = float(action_dist)

                next_state, reward, done, _ = step_env(env, [action])
                next_state = scaler.transform([next_state]).squeeze()
                next_state = next_state.reshape([1, state_size])

                total_reward += reward

                step_summary.value.add(tag='step_reward', simple_value=reward)
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action, reward=reward,
                                        next_state=next_state, done=done)
                episode_transitions.append(transition)

                state_val = critic.estimate(state, sess)
                next_state_val = 0 if done else critic.estimate(next_state, sess)

                target = reward + discount_factor * next_state_val
                advantage = target - state_val

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess)
                actor_loss = actor.fit(state, action, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                step_counter += 1

                summary_writer.add_summary(step_summary, step_counter)

                if done:
                    if total_reward < 0:
                        print("Episode {}, Reward: {:.2f}, Steps: {}, fail"
                              .format(episode, total_reward, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, actor, sess, scaler, evaluation_episodes)
                        episode_summary.value.add(tag='evaluation_average_rewards', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state
            minimum_episodes += 1

            if total_reward < 0 and minimum_episodes > 5:
                total_reward = 0
                minimum_episodes = 0

            if solved:
                saver = tf.train.Saver()
                saver.save(sess, 'cartToMount/cartToMount', global_step=step_counter)
                break

    summary_writer.flush()



if __name__ == '__main__':
    tf.reset_default_graph()

    sess = tf.Session()
    saver = tf.train.import_meta_graph("./cartpole/cartpole-10739.meta")
    saver.restore(sess, './cartpole/cartpole-10739')
    graph = tf.get_default_graph()

    actor_lr = 0.00008
    critic_lr = 0.004

    actor = ActorNetwork(learning_rate=actor_lr, graph=graph)
    critic = CriticNetwork(learning_rate=critic_lr, graph=graph)
    scaler = StandardScaler()

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, sess, actor, critic, scaler)
    print("--- %s seconds ---" % (time.time() - start_time))
    sess.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/CartpoleToMountainCar'

"""## **Section 3 – Transfer learning (50%)**

## {acrobot, mountainCar} -> cartPole
"""

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('CartPole-v1')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6

action_mapping = {0: 0, 2: 1}


def build_base(inputs, layer_name, graph, activation=None, name=None,
               kernel='kernel', bias='bias'):
    d1_w = graph.get_tensor_by_name(layer_name + '/' + kernel + ':0')
    d1_b = graph.get_tensor_by_name(layer_name + '/' + bias + ':0')
    output = tf.add(tf.matmul(inputs, d1_w), d1_b, name=name)

    if activation is None:
      return output
    else:
      return activation(output)


class ActorNetwork():
    def __init__(self, graph, learning_rate, name='actor_cp_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.advantage = tf.placeholder(tf.float32, [1], name='advantage')
            self.action = tf.placeholder(tf.float32, name="action")

            x = tf.layers.dense(inputs=self.state, units=12, activation=tf.nn.relu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')

            mc_h11 = build_base(inputs=self.state, layer_name='mountainCar/actor_mc_network/dense1',
                                activation=tf.nn.relu, graph=graph, name='mc_h11_output')

            mc_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(mc_h11), units=12, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='mc_adapter_h11')

            ac_h12 = build_base(inputs=self.state, layer_name='acrobot/acrobot_actor_network/base_net',
                                graph=graph, activation=tf.nn.relu, name='ac_h12_output')

            ac_adapter_h12 = tf.layers.dense(inputs=tf.stop_gradient(ac_h12), units=12, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h12')

            x = tf.layers.dense(inputs=x + 0.001 * mc_adapter_h11 + 0.001 * ac_adapter_h12, units=12,
                                activation=tf.nn.relu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense2')

            mc_h21 = build_base(inputs=mc_h11, layer_name='mountainCar/actor_mc_network/dense2',
                                activation=tf.nn.relu, graph=graph, name='mc_h21_output')

            mc_adapter_h21 = tf.layers.dense(inputs=tf.stop_gradient(mc_h21), units=12, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='mc_adapter_h21')

            ac_adapter_h22 = tf.layers.dense(inputs=tf.stop_gradient(ac_h12), units=12, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h22')

            self.base = tf.layers.dense(inputs=x + 0.001 * mc_adapter_h21 + 0.001 * ac_adapter_h22, units=12,
                                        activation=tf.nn.relu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')

            self.output = tf.layers.dense(inputs=self.base, units=action_size,
                                          activation=tf.nn.softmax,
                                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                          name='output')

            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.action)
            self.loss = tf.reduce_mean(neg_log_prob * self.advantage)

            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                                    name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess=sess)[0]

    def fit(self, state, action, advantage, sess) -> float:
        _, loss = sess.run([self.optimizer, self.loss],
                           {self.state: state,
                            self.action: action,
                            self.advantage: advantage})

        return loss


class CriticNetwork():
    def __init__(self, learning_rate, graph: tf.Graph, name='critic_cp_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.target = tf.placeholder(tf.float32, [1], name='target')

            x = tf.layers.dense(inputs=self.state, units=16, activation=tf.nn.relu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')

            mc_h11 = build_base(inputs=self.state, layer_name='mountainCar/critic_mc_network/dense1',
                                activation=tf.nn.relu, graph=graph, name='mc_h11_output')

            mc_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(mc_h11), units=16, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='mc_adapter_h11')

            ac_h11 = build_base(inputs=self.state, layer_name='acrobot/acrobot_critic_network/base_net',
                                graph=graph, activation=tf.nn.relu, name='ac_hl1_output')

            ac_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(ac_h11), units=16, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h11')

            self.base = tf.layers.dense(inputs=x + 0.001 * mc_adapter_h11 + 0.001 * ac_adapter_h11, units=8,
                                        activation=tf.nn.relu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')

            mc_h21 = build_base(inputs=mc_h11, layer_name='mountainCar/critic_mc_network/dense2',
                                activation=tf.nn.relu, graph=graph, name='mc_h21_output')

            mc_adapter_h21 = tf.layers.dense(inputs=tf.stop_gradient(mc_h21), units=8, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='mc_adapter_h21')

            ac_adapter_h22 = tf.layers.dense(inputs=tf.stop_gradient(ac_h11), units=8, activation=tf.nn.relu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h22')

            self.output = tf.layers.dense(inputs=self.base + 0.001 * mc_adapter_h21 + 0.001 * ac_adapter_h22, units=1,
                                          activation=None, name='output')

            self.loss = tf.math.squared_difference(self.output, self.target)

            self.training_op = tf.train.AdamOptimizer(learning_rate,
                                                      name='critic_optimizer').minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target})

        return loss


def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action):
    action = action_mapping[action] if action in action_mapping else env.action_space.sample()
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state: np.ndarray):
    return np.pad(state, (0, state_size - state.shape[0]))


def sample_space(env):
    return adapt_state(env.observation_space.sample())


def run_evaluate(env, actor, episodes, sess=None):
    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = state.reshape([1, state_size])
        episode_transitions = []

        for _ in range(env.spec.max_episode_steps):
            actions_distribution = actor.estimate(state, sess=sess)
            action: int = np.argmax(actions_distribution)

            next_state, reward, done, _ = step_env(env, action)
            next_state = next_state.reshape([1, state_size])

            action_one_hot = np.zeros(action_size)
            action_one_hot[action] = 1
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action_one_hot, reward=reward,
                                    next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state
          
    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))

    return average_rewards


def run(env, sess, scaler, actor, critic):
    observation_examples = np.array([sample_space(env) for _ in range(10000)])
    scaler.fit(observation_examples)

    max_episodes = 1500
    min_episodes_queue = 100
    discount_factor = 0.99
    evaluation_episodes = 100

    summary_writer = tf.summary.FileWriter('./logs/acrobot_mountaincar_to_cartpole')

    step_counter = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with sess.as_default():
        sess.run(tf.global_variables_initializer())

        total_reward = 0
        minimum_episodes = 0
        for episode in range(max_episodes):

            state = reset_env(env)
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                actions_distribution = actor.estimate(state, sess=sess)
                action = np.random.choice(np.arange(len(actions_distribution)), p=actions_distribution)
                next_state, reward, done, _ = step_env(env, action)
                next_state = next_state.reshape([1, state_size])

                step_summary.value.add(tag='step_reward', simple_value=reward)
                action_one_hot = np.zeros(action_size)
                action_one_hot[action] = 1
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action_one_hot, reward=reward,
                                        next_state=next_state, done=done)
                episode_transitions.append(transition)

                state_val = critic.estimate(state, sess)
                next_state_val = 0 if done else critic.estimate(next_state, sess)

                target = reward + discount_factor * next_state_val
                advantage = target - state_val

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess)
                actor_loss = actor.fit(state, action_one_hot, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                step_counter += 1

                summary_writer.add_summary(step_summary, step_counter)

                if done:
                    if total_reward < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, fail"
                              .format(episode, total_reward, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, actor, episodes=evaluation_episodes, sess=sess)
                        episode_summary.value.add(tag='evaluation_average_rewards', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state
                minimum_episodes += 1

            if total_reward < 0 and minimum_episodes > 5:
                total_reward = 0
                minimum_episodes = 0

            if solved:
                saver = tf.train.Saver()
                saver.save(sess, 'acrobot_mountaincar_to_cartpole/acrobot_mountaincar_to_cartpole',
                           global_step=step_counter)
                break

    summary_writer.flush()



if __name__ == '__main__':
    tf.reset_default_graph()
    actor_lr = 0.0002
    critic_lr = 0.01

    sess = tf.Session()
    mountaincar_cont_saver = tf.train.import_meta_graph("./mountain-car/mountain-car-24144.meta", import_scope='mountainCar')
    mountaincar_cont_saver.restore(sess, './mountain-car/mountain-car-24144')
    acrobot_saver = tf.train.import_meta_graph("./acrobot/acrobot-2327.meta", import_scope='acrobot')
    acrobot_saver.restore(sess, './acrobot/acrobot-2327')

    graph = tf.get_default_graph()

    scaler = StandardScaler()
    actor = ActorNetwork(learning_rate=actor_lr, graph=graph)
    critic = CriticNetwork(learning_rate=critic_lr, graph=graph)


    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    print("--- %s start time ---" % (current_time))
    run(env, sess, scaler, actor, critic)
    print("--- %s seconds ---" % (time.time() - start_time))
    sess.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/acrobot_mountaincar_to_cartpole'

"""## {acrobot, cartPole} -> mountainCar"""

# optimized for Tf2
tf.disable_v2_behavior()
print("tf_ver:{}".format(tf.__version__))

env = gym.make('MountainCarContinuous-v0')
np.random.seed(1)

Epsilon = 1e-5
action_size = 3
state_size = 6


def build_base(inputs, layer_name, graph, activation=None, name=None,
               kernel='kernel', bias='bias'):
    d1_w = graph.get_tensor_by_name(layer_name + '/' + kernel + ':0')
    d1_b = graph.get_tensor_by_name(layer_name + '/' + bias + ':0')
    output = tf.add(tf.matmul(inputs, d1_w), d1_b, name=name)

    if activation is None:
        return output
    else:
        return activation(output)


class ActorNetwork():
    def __init__(self, graph, learning_rate, name='actor_transMC_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.advantage = tf.placeholder(tf.float32, [1], name='advantage')
            self.action = tf.placeholder(tf.float32, name="action")

            x = tf.layers.dense(inputs=self.state, units=40, activation=tf.nn.elu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')

            cp_h11 = build_base(inputs=self.state, layer_name='cartpole/actor_network',
                                kernel='W1', bias='b1', activation=tf.nn.elu, graph=graph,
                                name='cp_h11_output')

            cp_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(cp_h11), units=40, activation=tf.nn.elu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='cp_adapter_h11')

            ac_h11 = build_base(inputs=self.state, layer_name='acrobot/acrobot_actor_network/base_net',
                                graph=graph, activation=tf.nn.relu, name='ac_hl1_output')

            ac_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(ac_h11), units=40, activation=tf.nn.elu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h11')

            self.base = tf.layers.dense(inputs=x + 0.01 * cp_adapter_h11 + 0.01 * ac_adapter_h11, units=40,
                                        activation=tf.nn.elu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')

            sigma = tf.layers.dense(inputs=self.base, units=1, activation=None, name='sigma')
            sigma = tf.nn.softplus(sigma) + Epsilon

            mu = tf.layers.dense(inputs=self.base, units=1, activation=None, name='mu')

            actions_distribution = tf.distributions.Normal(mu, sigma)
            _action = actions_distribution.sample()
            self.output = tf.clip_by_value(_action, -1, 1)
            self.loss = -tf.log(actions_distribution.prob(self.action) + Epsilon) * self.advantage

            self.training_op = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                                      name='actor_optimizer').minimize(self.loss)

    def predict(self, state, sess):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess):
        return self.predict(state, sess=sess)[0, 0]

    def fit(self, state, action, adv, sess):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.action: action,
                            self.advantage: adv})

        return loss


class CriticNetwork():
    def __init__(self, learning_rate, graph: tf.Graph, name='critic_transMC_network'):
        with tf.variable_scope(name):
            self.state = tf.placeholder(tf.float32, [None, state_size], name="state")
            self.target = tf.placeholder(tf.float32, [1], name='target')

            x = tf.layers.dense(inputs=self.state, units=400, activation=tf.nn.elu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense1')

            cp_h11 = build_base(inputs=self.state, layer_name='cartpole/critic_network/dense1',
                                activation=tf.nn.elu, graph=graph, name='cp_h11_output')

            cp_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(cp_h11), units=400, activation=tf.nn.elu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='cp_adapter_h11')

            ac_h11 = build_base(inputs=self.state, layer_name='acrobot/acrobot_critic_network/base_net',
                                graph=graph, activation=tf.nn.relu, name='ac_hl1_output')

            ac_adapter_h11 = tf.layers.dense(inputs=tf.stop_gradient(ac_h11), units=400, activation=tf.nn.elu,
                                             kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                             name='ac_adapter_h11')

            x = tf.layers.dense(inputs=x + 0.001 * cp_adapter_h11 + 0.001 * ac_adapter_h11, units=400,
                                activation=tf.nn.elu,
                                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                name='dense2')

            self.base = tf.layers.dense(inputs=x + 0.001 * cp_adapter_h11 + 0.001 * ac_adapter_h11, units=400,
                                        activation=tf.nn.elu,
                                        kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),
                                        name='base_net')

            self.output = tf.layers.dense(inputs=self.base, units=1, activation=None, name='output')

            self.loss = tf.math.squared_difference(self.output, self.target)

            self.training_op = tf.train.AdamOptimizer(learning_rate,
                                                      name='critic_optimizer').minimize(self.loss)

    def predict(self, state, sess: tf.Session):
        return sess.run(self.output, {self.state: state})

    def estimate(self, state, sess: tf.Session):
        return self.predict(state, sess)[0, 0]

    def fit(self, state, target, sess: tf.Session):
        _, loss = sess.run([self.training_op, self.loss],
                           {self.state: state,
                            self.target: target})

        return loss


def reset_env(env):
    return adapt_state(env.reset())


def step_env(env, action):
    state, reward, done, info = env.step(action)
    state = state.squeeze()
    return adapt_state(state), reward, done, info


def adapt_state(state: np.ndarray):
    return np.pad(state, (0, state_size - state.shape[0]))


def sample_space(env):
    return adapt_state(env.observation_space.sample())


def run_evaluate(env, scaler, actor, episodes, sess):
    episode_rewards = []
    average_rewards = float('-inf')
    for i in range(episodes):

        state = reset_env(env)
        state = scaler.transform([state]).squeeze()
        state = state.reshape([1, state_size])
        episode_transitions = []

        for _ in range(env.spec.max_episode_steps):
            action_dist = actor.predict(state, sess)
            action = float(action_dist)

            next_state, reward, done, _ = step_env(env, [action])
            next_state = scaler.transform([next_state]).squeeze()
            next_state = next_state.reshape([1, state_size])
            Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
            transition = Transition(state=state, action=action, reward=reward,
                                    next_state=next_state, done=done)
            episode_transitions.append(transition)

            if done:
                episode_reward = sum(t.reward for t in episode_transitions)
                episode_rewards.append(episode_reward)
                average_rewards = mean(episode_rewards)

                break
            state = next_state

    print("Evaluation Episodes Summary - Average over {} episodes: {:.2f}".format(episodes, average_rewards))

    return average_rewards


def run(env, sess, scaler, actor, critic):
    observation_examples = np.array([sample_space(env) for _ in range(10000)])
    scaler.fit(observation_examples)

    max_episodes = 500
    min_episodes_queue = 100
    discount_factor = 0.99
    evaluation_episodes = 100

    summary_writer = tf.summary.FileWriter('./logs/acrobot_cartpole_to_mountaincar')

    step_counter = 0
    solved = False
    episode_rewards = deque(maxlen=min_episodes_queue)

    with sess.as_default():
        sess.run(tf.global_variables_initializer())

        total_reward = 0
        minimum_episodes = 0
        for episode in range(max_episodes):

            state = reset_env(env)
            state = scaler.transform([state]).squeeze()
            state = state.reshape([1, state_size])
            episode_transitions = []
            I = 1

            for step in range(env.spec.max_episode_steps):
                step_summary = tf.Summary()

                action_dist = actor.predict(state, sess)
                action = float(action_dist)

                next_state, reward, done, _ = step_env(env, [action])
                next_state = scaler.transform([next_state]).squeeze()
                next_state = next_state.reshape([1, state_size])

                total_reward += reward

                step_summary.value.add(tag='step_reward', simple_value=reward)
                Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
                transition = Transition(state=state, action=action, reward=reward,
                                        next_state=next_state, done=done)
                episode_transitions.append(transition)

                state_val = critic.estimate(state, sess)
                next_state_val = 0 if done else critic.estimate(next_state, sess)

                target = reward + discount_factor * next_state_val
                advantage = target - state_val

                target = np.reshape(target, (-1,))
                advantage = np.reshape(advantage, (-1,))

                critic_loss = critic.fit(state, target * I, sess)
                actor_loss = actor.fit(state, action, advantage * I, sess)

                step_summary.value.add(tag='critic_loss', simple_value=critic_loss)
                step_summary.value.add(tag='actor_loss', simple_value=actor_loss)

                step_counter += 1

                summary_writer.add_summary(step_summary, step_counter)

                if done:
                    if total_reward < 0:
                        print("Episode {},Reward: {:.2f}, Steps: {}, fail"
                              .format(episode, total_reward, step + 1))
                        break
                    episode_reward = sum(t.reward for t in episode_transitions)
                    episode_rewards.append(episode_reward)
                    average_rewards = mean(episode_rewards)
                    solved = episode_reward > env.spec.reward_threshold

                    episode_summary = tf.Summary()
                    episode_summary.value.add(tag='reward_per_episode', simple_value=episode_reward)
                    episode_summary.value.add(tag='average_rewards', simple_value=average_rewards)

                    print("Episode {}, Reward: {:.2f}, Steps: {}, Average over {} episodes: {:.2f}"
                          .format(episode, episode_reward, step + 1, min_episodes_queue, average_rewards))

                    if solved:
                        evaluate = run_evaluate(env, scaler, actor, evaluation_episodes, sess)
                        episode_summary.value.add(tag='evaluation_average_rewards', simple_value=evaluate)
                        solved = evaluate > env.spec.reward_threshold

                    summary_writer.add_summary(episode_summary, episode)

                    break
                state = next_state
                minimum_episodes += 1

            if total_reward < 0 and minimum_episodes > 5:
                total_reward = 0
                minimum_episodes = 0

            if solved:
                saver = tf.train.Saver()
                saver.save(sess, 'acrobot_cartpole_to_mountaincar/acrobot_cartpole_to_mountaincar',
                           global_step=step_counter)
                break

    summary_writer.flush()


if __name__ == '__main__':
    tf.reset_default_graph()
    actor_lr = 0.00001
    critic_lr = 0.0005

    sess = tf.Session()
    cartpole_saver = tf.train.import_meta_graph("./cartpole/cartpole-25075.meta", import_scope='cartpole')
    cartpole_saver.restore(sess, './cartpole/cartpole-25075')
    acrobot_saver = tf.train.import_meta_graph("./acrobot/acrobot-2327.meta", import_scope='acrobot')
    acrobot_saver.restore(sess, './acrobot/acrobot-2327')

    graph = tf.get_default_graph()
    scaler = StandardScaler()
    actor = ActorNetwork(learning_rate=actor_lr, graph=graph)
    critic = CriticNetwork(learning_rate=critic_lr, graph=graph)

    start_time = time.time()
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")

    print("--- %s start time ---" % (current_time))
    run(env, sess, scaler, actor, critic)
    print("--- %s seconds ---" % (time.time() - start_time))
    sess.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='./logs/acrobot_cartpole_to_mountaincar'